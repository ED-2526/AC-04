import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import shutil
import warnings
import matplotlib
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, pairwise_distances
from sklearn.manifold import TSNE
from scipy.spatial.distance import cdist
from scipy.stats import pearsonr
from xgboost import XGBClassifier
from yellowbrick.cluster import KElbowVisualizer

# ==============================================================================
# 0. CONFIGURACI√ì INICIAL
# ==============================================================================
matplotlib.use('Agg')
warnings.filterwarnings('ignore')

# Crear carpeta de resultats
output_folders = {'silhouette': 'resultats_xgboost_silhouette'}

for folder in output_folders.values():
    if os.path.exists(folder):
        shutil.rmtree(folder)
    os.makedirs(folder)
    print(f"Carpeta '{folder}' preparada.")

# ==============================================================================
# 1. C√ÄRREGA I PREPROCESSAMENT DE DADES
# ==============================================================================
print("\n--- 1. C√†rrega i Preprocessament de Dades ---")
filename = 'marketing_campaign.csv'

try:
    data = pd.read_csv(filename, sep="\t")
except FileNotFoundError:
    print(f"Error: No s'ha trobat '{filename}'.")
    exit()

# Feature Engineering B√†sic
data['Age'] = 2025 - data['Year_Birth']
data['Total_Spending'] = (
    data['MntWines'] + data['MntFruits'] +
    data['MntMeatProducts'] + data['MntFishProducts'] +
    data['MntSweetProducts'] + data['MntGoldProds']
)

# Eliminaci√≥ Outliers Spending
Q1_spend = data['Total_Spending'].quantile(0.25)
Q3_spend = data['Total_Spending'].quantile(0.75)
IQR_spend = Q3_spend - Q1_spend
data = data[data['Total_Spending'] <= (Q3_spend + 2.0 * IQR_spend)]

# Variables Familiars
partner_status = ['Married', 'Together']
data['Has_Partner'] = data['Marital_Status'].apply(lambda x: 1 if x in partner_status else 0)
data['Family_Size'] = 1 + data['Has_Partner'] + data['Kidhome'] + data['Teenhome']

# Antiguitat Client
data['Dt_Customer'] = pd.to_datetime(data['Dt_Customer'], dayfirst=True)
data['Tenure_Days'] = (data['Dt_Customer'].max() - data['Dt_Customer']).dt.days

# Neteja B√†sica
data = data.dropna(subset=['Income'])
data = data[(data['Age'] < 100) & (data['Income'] < 600000)]
invalid_status = ['YOLO', 'Absurd', 'Alone']
data = data[~data['Marital_Status'].isin(invalid_status)]

# Outliers Income
Q1_inc = data['Income'].quantile(0.25)
Q3_inc = data['Income'].quantile(0.75)
data = data[data['Income'] <= (Q3_inc + 1.5 * (Q3_inc - Q1_inc))]

# One-Hot Encoding (per an√†lisi posterior, no per clustering)
education_dummies = pd.get_dummies(data['Education'], prefix='Edu', drop_first=True)
marital_dummies = pd.get_dummies(data['Marital_Status'], prefix='Marital', drop_first=True)
data = pd.concat([data, education_dummies, marital_dummies], axis=1)

# Prefer√®ncies de producte (spending ratios)
epsilon = 1e-6
data['Wine_Ratio'] = data['MntWines'] / (data['Total_Spending'] + epsilon)
data['Meat_Ratio'] = data['MntMeatProducts'] / (data['Total_Spending'] + epsilon)
data['Sweet_Ratio'] = data['MntSweetProducts'] / (data['Total_Spending'] + epsilon)
data['Fish_Ratio'] = data['MntFishProducts'] / (data['Total_Spending'] + epsilon)
data['Fruit_Ratio'] = data['MntFruits'] / (data['Total_Spending'] + epsilon)
data['Gold_Ratio'] = data['MntGoldProds'] / (data['Total_Spending'] + epsilon)

print(f"Dades netes: {len(data)} registres.")

# ==============================================================================
# 2. SELECCI√ì DE VARIABLES PER CLUSTERING
# ==============================================================================
selected_columns = [
    # Variables de comportament de compra
    'Income', 'Total_Spending', 
    'MntWines', 'MntMeatProducts', 'MntFishProducts',
    'MntFruits', 'MntSweetProducts', 'MntGoldProds',
    
    # Prefer√®ncies de producte (ratios)
    'Wine_Ratio', 'Meat_Ratio', 'Sweet_Ratio', 
    'Fish_Ratio', 'Fruit_Ratio', 'Gold_Ratio',
    
    # Engagement i context
    'Tenure_Days', 'Family_Size', 'Age'
]

X = data[selected_columns].values
cols = selected_columns

print(f"\nVariables seleccionades per clustering ({len(cols)}):")
for i, col in enumerate(cols, 1):
    print(f"  {i}. {col}")

# ==============================================================================
# 3. DETERMINACI√ì K √íPTIMA (M√àTODE DEL COLZE)
# ==============================================================================
print("\n--- 2. Determinant K √íptima amb M√®tode del Colze ---")

scaler_elbow = StandardScaler()
X_scaled_elbow = scaler_elbow.fit_transform(X)

folder = output_folders['silhouette']
print(f"  Generant Elbow Plot (KMeans)...")

plt.figure(figsize=(10, 6))
visualizer = KElbowVisualizer(
    KMeans(random_state=42, n_init=10), 
    k=(2, 10), 
    metric='distortion',
    timings=False
)
visualizer.fit(X_scaled_elbow)
optimal_k = visualizer.elbow_value_
print(f"  K √≤ptima detectada: {optimal_k}")
visualizer.show(outpath=os.path.join(folder, f'00_elbow_kmeans.png'))
plt.close()

# ==============================================================================
# 4. AN√ÄLISI D'IMPORT√ÄNCIA AMB XGBOOST
# ==============================================================================
print("\n--- 3. Entrenament XGBoost per Feature Selection ---")

def get_xgboost_importance(X_input, column_names, scaler_name, optimal_k):
    """
    Calcula la import√†ncia de features mitjan√ßant XGBoost amb pseudo-labeling:
    1. Escala les dades
    2. Crea clusters temporals (KMeans) com a target
    3. Entrena XGBoost per predir aquests clusters
    4. Extreu les import√†ncies de features
    """
    scaler = MinMaxScaler() if scaler_name == 'minmax' else StandardScaler()
    X_scaled = scaler.fit_transform(X_input)
    
    # Pseudo-labeling: Clusters com a target artificial
    kmeans_base = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
    y_pseudo = kmeans_base.fit_predict(X_scaled)
    
    # Entrenar XGBoost
    model = XGBClassifier(
        n_estimators=100, 
        random_state=42,
        eval_metric='mlogloss',
        use_label_encoder=False
    )
    model.fit(X_scaled, y_pseudo)
    
    # Extreure import√†ncies
    importances = model.feature_importances_
    df_imp = pd.DataFrame({
        'Feature': column_names,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)
    
    return df_imp, model

# Calcular import√†ncies per ambd√≥s scalers
print("  Calculant import√†ncies (Standard Scaler)...")
imp_standard, model_std = get_xgboost_importance(X, cols, 'standard', optimal_k)

print("  Calculant import√†ncies (MinMax Scaler)...")
imp_minmax, model_mm = get_xgboost_importance(X, cols, 'minmax', optimal_k)

# Guardar informes
txt_path = os.path.join(folder, 'resultats_xgboost_importance.txt')
with open(txt_path, 'w', encoding='utf-8') as f:
    f.write(f"INFORME: XGBOOST FEATURE IMPORTANCE (Pseudo-Labeling K={optimal_k})\n")
    f.write("=========================================================\n\n")
    f.write("R√ÄNQUING (Standard Scaler):\n")
    f.write(imp_standard.to_string(index=False))
    f.write("\n\n")
    f.write("R√ÄNQUING (MinMax Scaler):\n")
    f.write(imp_minmax.to_string(index=False))

# Gr√†fic d'import√†ncia
plt.figure(figsize=(10, 6))
sns.barplot(data=imp_standard, x='Importance', y='Feature', palette='viridis')
plt.title('XGBoost Feature Importance (Standard Scaler)')
plt.tight_layout()
plt.savefig(os.path.join(folder, 'xgboost_feature_importance.png'))
plt.close()

print(f"  Informes guardats a {folder}")

# ==============================================================================
# 5. AVALUACI√ì CLUSTERING AMB LLINDARS XGBOOST
# ==============================================================================
print("\n--- 4. Avaluant Llindars d'Import√†ncia XGBoost ---")

def evaluate_xgboost_thresholds(X_original, column_names, imp_df_std, imp_df_minmax, 
                                 k_clusters, clustering_method='kmeans'):
    """
    Avalua diferents llindars d'import√†ncia XGBoost per selecci√≥ de variables.
    Retorna un DataFrame amb scores per cada combinaci√≥ scaler-threshold.
    """
    thresholds = [0.001, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.07, 0.10]
    scalers = ['minmax', 'standard']
    results = []
    
    for scaler_name in scalers:
        # Seleccionar import√†ncies segons scaler
        current_imp_df = imp_minmax if scaler_name == 'minmax' else imp_standard
        
        # Escalar dades
        scaler = MinMaxScaler() if scaler_name == 'minmax' else StandardScaler()
        X_scaled_global = scaler.fit_transform(X_original)
        
        for th in thresholds:
            # Seleccionar variables amb import√†ncia >= threshold
            selected_vars = current_imp_df[current_imp_df['Importance'] >= th]['Feature'].tolist()
            
            if len(selected_vars) < 2:
                continue
            
            selected_indices = [column_names.index(v) for v in selected_vars]
            X_subset = X_scaled_global[:, selected_indices]
            
            try:
                # Executar clustering
                if clustering_method == 'kmeans':
                    model = KMeans(n_clusters=k_clusters, random_state=42, n_init=10)
                elif clustering_method == 'gmm':
                    model = GaussianMixture(n_components=k_clusters, random_state=42)
                elif clustering_method == 'hierarchical':
                    model = AgglomerativeClustering(n_clusters=k_clusters)
                elif clustering_method == 'spectral':
                    model = SpectralClustering(n_clusters=k_clusters, random_state=42, 
                                              affinity='nearest_neighbors')
                
                labels = model.fit_predict(X_subset)
                
                # Calcular Silhouette
                if len(set(labels)) >= 2:
                    score = silhouette_score(X_subset, labels)
                    
                    # Penalitzaci√≥ si hi ha variables categ√≤riques
                    bad_keywords = ['Marital', 'Education', 'Edu']
                    has_bad_var = any(any(bk in var_name for bk in bad_keywords) 
                                     for var_name in selected_vars)
                    if has_bad_var:
                        score = score - 0.1
                else:
                    continue
                    
                results.append({
                    'scaler': scaler_name,
                    'threshold': th,
                    'num_vars': len(selected_vars),
                    'vars': ", ".join(selected_vars),
                    'score': score
                })
            except Exception:
                continue
            
    return pd.DataFrame(results)

# Processar nom√©s Silhouette
clustering_methods = ['kmeans', 'gmm', 'hierarchical', 'spectral']
metric_name = 'silhouette'
metric_config = {
    'folder': output_folders['silhouette'], 
    'ylabel': 'Silhouette Score', 
    'better': 'higher'
}

all_results = {metric_name: {}}

print(f"\n{'='*60}")
print(f"PROCESSANT M√àTRICA: {metric_name.upper()} (VIA XGBOOST)")
print(f"{'='*60}")

for method in clustering_methods:
    print(f"  Calculant {method.upper()} amb {metric_name}...")
    
    df_eval = evaluate_xgboost_thresholds(
        X, cols, imp_standard, imp_minmax, 
        k_clusters=optimal_k,
        clustering_method=method
    )
    
    all_results[metric_name][method] = df_eval
    
    # Guardar CSV
    csv_path = os.path.join(folder, f'evaluacio_xgboost_variables_{metric_name}_{method}.csv')
    df_eval.to_csv(csv_path, index=False)

print(f"\nC√†lculs finalitzats. CSVs guardats.")

# ==============================================================================
# 6. GR√ÄFICS COMPARATIUS
# ==============================================================================
print("\n--- 5. Generant Gr√†fics Comparatius ---")

ylabel = metric_config['ylabel']

for method in clustering_methods:
    df_eval = all_results[metric_name][method]
    
    if df_eval.empty:
        print(f"    Warning: No dades per {method}")
        continue
    
    plt.figure(figsize=(10, 6))
    sns.set_style("whitegrid")
    
    # Gr√†fic Score vs Threshold
    sns.lineplot(data=df_eval, x='threshold', y='score', hue='scaler', 
                 marker='o', linewidth=2.5, palette=['blue', 'red'])
    
    plt.title(f'{ylabel} vs XGB Importance Threshold - {method.upper()}', fontsize=14)
    plt.ylabel(ylabel, fontsize=12)
    plt.xlabel('XGB Importance Threshold (> X)', fontsize=12)
    
    # Anotacions nombre de variables
    for i in range(df_eval.shape[0]):
        row = df_eval.iloc[i]
        plt.text(row['threshold'], row['score'], f"v={int(row['num_vars'])}", 
                 ha='center', va='bottom', size='small', weight='bold', rotation=45)
    
    plt.tight_layout()
    plt.savefig(os.path.join(folder, f'grafic_comparativa_{metric_name}_{method}.png'))
    plt.close()
    
    print(f"    ‚úì {method}")

# ==============================================================================
# 7. VISUALITZACI√ì MILLORS RESULTATS (PCA + t-SNE)
# ==============================================================================
print("\n--- 6. Visualitzant Millors Resultats amb PCA i t-SNE ---")

def get_best_result(df_results, method_name, metric_name, better='higher'):
    """Retorna la millor configuraci√≥ amb m√≠nim 2 variables"""
    if df_results.empty:
        return None
    
    df_filtered = df_results[df_results['num_vars'] >= 2]
    if df_filtered.empty:
        return None
    
    if better == 'higher':
        return df_filtered.loc[df_filtered['score'].idxmax()]
    else:
        return df_filtered.loc[df_filtered['score'].idxmin()]

def apply_clustering(X_subset, method, k_clusters=4):
    """Aplica el m√®tode de clustering especificat"""
    if method == 'kmeans':
        model = KMeans(n_clusters=k_clusters, random_state=42, n_init=10)
    elif method == 'gmm':
        model = GaussianMixture(n_components=k_clusters, random_state=42)
    elif method == 'hierarchical':
        model = AgglomerativeClustering(n_clusters=k_clusters)
    elif method == 'spectral':
        model = SpectralClustering(n_clusters=k_clusters, random_state=42, 
                                   affinity='nearest_neighbors')
    
    return model.fit_predict(X_subset)

def visualize_best_clustering(X_original, column_names, best_config, method_name, 
                              metric_name, folder):
    """Genera visualitzacions PCA i t-SNE per la millor configuraci√≥"""
    if best_config is None:
        return
    
    print(f"    - Scaler: {best_config['scaler']}")
    print(f"    - Threshold: {best_config['threshold']}")
    print(f"    - Variables: {best_config['vars']}")
    print(f"    - Score: {best_config['score']:.4f}")
    
    # Preparar dades
    scaler = MinMaxScaler() if best_config['scaler'] == 'minmax' else StandardScaler()
    X_scaled_full = scaler.fit_transform(X_original)
    
    # Seleccionar variables via XGBoost Importance
    imp_df = imp_minmax if best_config['scaler'] == 'minmax' else imp_standard
    selected_vars = imp_df[imp_df['Importance'] >= best_config['threshold']]['Feature'].tolist()
    selected_indices = [column_names.index(v) for v in selected_vars]
    
    if len(selected_indices) == 0:
        print(f"    [WARNING] No features selected. Skipping visualization.")
        return

    X_subset = X_scaled_full[:, selected_indices]
    labels = apply_clustering(X_subset, method_name, k_clusters=4)
    
    # Visualitzaci√≥
    n_viz_pca = min(2, X_subset.shape[1])
    pca_viz = PCA(n_components=n_viz_pca)
    X_pca = pca_viz.fit_transform(X_subset)
    
    plt.figure(figsize=(12, 5))
    
    # PCA Plot
    plt.subplot(1, 2, 1)
    if X_pca.shape[1] >= 2:
        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', 
                              alpha=0.6, edgecolors='k', linewidth=0.5)
        plt.xlabel(f'PC1 ({pca_viz.explained_variance_ratio_[0]*100:.1f}%)')
        plt.ylabel(f'PC2 ({pca_viz.explained_variance_ratio_[1]*100:.1f}%)')
    else:
        scatter = plt.scatter(X_pca[:, 0], np.zeros_like(X_pca[:, 0]), c=labels, 
                            cmap='viridis', alpha=0.6, edgecolors='k', linewidth=0.5)
        plt.xlabel('PC1')
        plt.ylabel('Fixed Axis')
        
    plt.colorbar(scatter, label='Cluster')
    plt.title(f'PCA - {method_name.upper()}\nScore: {best_config["score"]:.4f}')
    plt.grid(True, alpha=0.3)
    
    # t-SNE Plot
    plt.subplot(1, 2, 2)
    perp = min(30, len(X_subset) - 1)
    tsne = TSNE(n_components=2, random_state=42, perplexity=perp)
    X_tsne = tsne.fit_transform(X_subset)
    
    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='viridis', 
                          alpha=0.6, edgecolors='k', linewidth=0.5)
    plt.colorbar(scatter, label='Cluster')
    plt.xlabel('t-SNE 1')
    plt.ylabel('t-SNE 2')
    plt.title(f't-SNE - {method_name.upper()}\nvars={len(selected_vars)}')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(folder, f'viz_best_{method_name}_xgboost_tsne.png'), dpi=150)
    plt.close()

# Processar visualitzacions
better = metric_config['better']

for method in clustering_methods:
    df_results = all_results[metric_name][method]
    best_config = get_best_result(df_results, method, metric_name, better)
    
    if best_config is not None:
        print(f"  {method.upper()}:")
        visualize_best_clustering(X, cols, best_config, method, metric_name, folder)

# ==============================================================================
# 8. PERFIL COMPLET DELS CL√öSTERS AMB M√àTRIQUES ADICIONALS
# ==============================================================================
print("\n--- 7. Perfil complet dels cl√∫sters (millor cas per m√®tode) ---")

def calculate_extra_metrics(X_subset, labels):
    """
    Calcula m√®triques adicionals de qualitat del clustering:
    - SSE Normalitzat: Cohesi√≥ intra-cluster
    - BSS Normalitzat: Separaci√≥ inter-cluster
    - Correlaci√≥ Pearson: Relaci√≥ incid√®ncia-proximitat
    """
    unique_labels = np.unique(labels)
    centers = np.array([X_subset[labels == i].mean(axis=0) for i in unique_labels])
    global_mean = X_subset.mean(axis=0)
    
    # SSE (Within-cluster Sum of Squares)
    sse = 0
    for i, label in enumerate(unique_labels):
        cluster_points = X_subset[labels == label]
        sse += np.sum(cdist(cluster_points, [centers[i]])**2)
    
    # SST (Total Sum of Squares)
    sst = np.sum((X_subset - global_mean)**2)
    
    # BSS (Between-cluster Sum of Squares)
    bss = sst - sse
    
    # Normalitzar per SST
    sse_norm = sse / sst
    bss_norm = bss / sst
    
    # Correlaci√≥ (mostra si punts propers estan al mateix cluster)
    idx = np.random.choice(len(X_subset), min(len(X_subset), 1000), replace=False)
    X_sample = X_subset[idx]
    labels_sample = labels[idx]
    
    incidence_matrix = (labels_sample[:, None] == labels_sample[None, :]).astype(int)
    dist_matrix = pairwise_distances(X_sample)
    
    corr, _ = pearsonr(incidence_matrix.flatten(), dist_matrix.flatten())
    
    return sse_norm, bss_norm, corr

def print_and_save_cluster_profile_full(data_df, labels, case_name, out_folder, extra_metrics):
    """
    Imprimeix i guarda perfil complet dels cl√∫sters amb m√®triques de qualitat
    """
    tmp = data_df.copy()
    tmp['_CLUSTER_'] = labels

    # Variables num√®riques per perfil
    num_cols = tmp.select_dtypes(include=[np.number]).columns.tolist()
    num_cols = [c for c in num_cols if c != '_CLUSTER_']

    summary = tmp.groupby('_CLUSTER_')[num_cols].mean().round(2)
    counts = tmp['_CLUSTER_'].value_counts().sort_index()
    summary['Count (Clients)'] = counts.values

    sse_n, bss_n, corr = extra_metrics

    # Header amb m√®triques
    metrics_header = (
        f"\n{'='*90}\n"
        f"üìä PERFIL COMPLET: {case_name}\n"
        f"{'-'*90}\n"
        f"   >> BSS (Separaci√≥) Norm: {bss_n:.2%}  (M√©s alt millor)\n"
        f"   >> SSE (Cohesi√≥) Norm:   {sse_n:.2%}  (M√©s baix millor)\n"
        f"   >> Correlaci√≥ (Pearson): {corr:.4f}   (M√©s a prop de -1 millor)\n"
        f"{'='*90}"
    )

    print(metrics_header)
    print(summary.T)

    # Guardar arxius
    safe_name = case_name.lower().replace(" ", "_").replace("/", "_").replace("|", "")
    csv_path = os.path.join(out_folder, f'perfil_clusters_{safe_name}.csv')
    txt_path = os.path.join(out_folder, f'perfil_clusters_{safe_name}.txt')

    summary.T.to_csv(csv_path, sep='\t')

    with open(txt_path, "w", encoding="utf-8") as f:
        f.write(metrics_header + "\n\n")
        f.write(summary.T.to_string())
        f.write("\n")

    print(f"   -> Guardat: {csv_path}")
    print(f"   -> Guardat: {txt_path}")

def build_labels_and_metrics_for_best_config(X_original, column_names, best_config, 
                                             method_name, k_clusters):
    """
    Reprodueix la millor configuraci√≥ i calcula labels + m√®triques
    """
    scaler = MinMaxScaler() if best_config['scaler'] == 'minmax' else StandardScaler()
    X_scaled_full = scaler.fit_transform(X_original)

    imp_df = imp_minmax if best_config['scaler'] == 'minmax' else imp_standard
    selected_vars = imp_df[imp_df['Importance'] >= best_config['threshold']]['Feature'].tolist()
    selected_indices = [column_names.index(v) for v in selected_vars]
    
    if len(selected_indices) == 0:
        return None, None

    X_subset = X_scaled_full[:, selected_indices]
    labels = apply_clustering(X_subset, method_name, k_clusters=k_clusters)
    extra_metrics = calculate_extra_metrics(X_subset, labels)
    
    return labels, extra_metrics

# Generar perfils per cada m√®tode
for method in clustering_methods:
    df_results = all_results[metric_name][method]
    best_config = get_best_result(df_results, method, metric_name, better)

    if best_config is None:
        print(f"  ‚ö†Ô∏è Sense millor configuraci√≥ per {method.upper()}.")
        continue

    best_labels, extra_metrics = build_labels_and_metrics_for_best_config(
        X_original=X,
        column_names=cols,
        best_config=best_config,
        method_name=method,
        k_clusters=optimal_k
    )
    
    if best_labels is None:
        print(f"  ‚ö†Ô∏è Error recalculant labels per {method.upper()}.")
        continue

    case_name = (f"XGBOOST BEST | {method.upper()} | "
                f"scaler={best_config['scaler']} | "
                f"th={best_config['threshold']} | k={optimal_k}")
    
    print_and_save_cluster_profile_full(data, best_labels, case_name, folder, extra_metrics)

# ==============================================================================
# RESUM FINAL
# ==============================================================================
print("\n" + "="*60)
print("PROC√âS COMPLETAT!")
print("="*60)
print(f"\nüìÅ Arxius generats a '{output_folders['silhouette']}':")
print("  ‚úì Informe Feature Importance XGBoost (TXT)")
print("  ‚úì Gr√†fic Feature Importance (PNG)")
print("  ‚úì M√®tode del colze per K √≤ptima (PNG)")
print("  ‚úì 4 gr√†fics comparatius amb SILHOUETTE (PNG)")
print("  ‚úì 4 visualitzacions PCA+t-SNE millors resultats (PNG)")
print("  ‚úì 4 perfils complets dels cl√∫sters amb m√®triques (TXT + CSV)")
print("  ‚úì CSVs amb avaluacions per cada m√®tode")
print("\n" + "="*60)